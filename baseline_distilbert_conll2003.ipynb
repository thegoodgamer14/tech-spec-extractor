{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMQNMSmQA67Ohr2Ln50ns3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegoodgamer14/tech-spec-extractor/blob/main/baseline_distilbert_conll2003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lG0rUyKIGehG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc511819-bb00-49c1-a292-76a6111fdbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tech-spec-extractor' already exists and is not an empty directory.\n",
            "/content/tech-spec-extractor/notebooks\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thegoodgamer14/tech-spec-extractor.git\n",
        "%cd tech-spec-extractor/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/tech-spec-extractor')"
      ],
      "metadata": {
        "id": "vOwZ1rmR4H8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnFpYweO4Vhy",
        "outputId": "23fd585e-2bb4-4ea4-cba7-d9dd013b1680"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/tech-spec-extractor/data'\n",
        "MODELS_DIR = '/content/drive/MyDrive/tech-spec-extractor/models'\n",
        "RESULTS_DIR = '/content/drive/MyDrive/tech-spec-extractor/results'"
      ],
      "metadata": {
        "id": "poUtipG04ejP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install numpy as np\n",
        "!pip install pypdf2\n",
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "BrmGFmtGJWYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "pi6gjCx-5QEI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "R9Jg0BJMHTaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CoNLL-2003 dataset...\")\n",
        "try:\n",
        "    conll_dataset = load_dataset(\"conll2003\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    raise e\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "print(conll_dataset)\n",
        "\n",
        "label_list = conll_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(f\"\\nNER Tags: {label_list}\")\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "print(\"\\nSample from training data:\")\n",
        "sample = conll_dataset[\"train\"][0]\n",
        "print(\"Tokens:\", sample[\"tokens\"][:10])\n",
        "print(\"NER tags:\", [label_list[tag] for tag in sample[\"ner_tags\"][:10]])"
      ],
      "metadata": {
        "id": "0wmdo2iw5XJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = conll_dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for i, label in enumerate(label_list)}\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(f\"\\nNER Tags: {label_list}\")\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "print(\"\\nSample from training data:\")\n",
        "sample = conll_dataset[\"train\"][0]\n",
        "print(\"Tokens:\", sample[\"tokens\"][:10])\n",
        "print(\"NER tags:\", [label_list[tag] for tag in sample[\"ner_tags\"][:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDcj94p-5qVU",
        "outputId": "ddaca3a5-6df8-4798-81ae-67373609d10b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NER Tags: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
            "Number of labels: 9\n",
            "\n",
            "Sample from training data:\n",
            "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "NER tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "print(f\"Loading tokenizer and model for: {model_checkpoint}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")\n",
        "print(\"Model and tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "id": "CZuRqdPHH6OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting up preprocessing function...\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Applying preprocessing to the dataset...\")\n",
        "tokenized_datasets = conll_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "print(\"Preprocessing complete.\")\n",
        "print(f\"Tokenized dataset structure: {tokenized_datasets}\")\n",
        "\n",
        "print(\"\\nSample from preprocessed training data:\")\n",
        "sample = tokenized_datasets[\"train\"][0]\n",
        "print(f\"Input IDs (first 10): {sample['input_ids'][:10]}\")\n",
        "print(f\"Attention Mask (first 10): {sample['attention_mask'][:10]}\")\n",
        "print(f\"Labels (first 10): {sample['labels'][:10]}\")"
      ],
      "metadata": {
        "id": "qC6NKwSlIBe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "print(\"Data collator set up successfully.\")\n",
        "\n",
        "print(\"Setting up evaluation metrics using 'evaluate' and 'seqeval'...\")\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "print(\"Evaluation metric function defined.\")\n",
        "\n",
        "print(\"Configuring training arguments...\")\n",
        "output_dir = \"./results_baseline\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_baseline',\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "print(\"Instantiating Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "print(\"Trainer instantiated successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IANVk89PIF4k",
        "outputId": "0ba4f95c-5825-4bbd-c741-e7fc5ace5323"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collator set up successfully.\n",
            "Setting up evaluation metrics using 'evaluate' and 'seqeval'...\n",
            "Evaluation metric function defined.\n",
            "Configuring training arguments...\n",
            "Training arguments configured.\n",
            "Instantiating Trainer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-cfea78f0acb4>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer instantiated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting baseline model fine-tuning...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training finished successfully!\")\n",
        "\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "    print(\"\\n--- Training Metrics ---\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during training: {e}\")\n",
        "    print(\"This might be due to GPU memory limits. Try reducing batch sizes.\")\n",
        "    raise e\n",
        "\n",
        "trainer.save_model(f\"{output_dir}/final_model\")\n",
        "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
        "print(f\"Baseline model saved to {output_dir}/final_model\")"
      ],
      "metadata": {
        "id": "PkLytsueIH7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the fine-tuned baseline model on the test set...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "\n",
        "print(\"\\n--- Baseline Model CoNLL-2003 Test Set Results ---\")\n",
        "print(f\"Precision: {eval_results.get('eval_precision', 'N/A'):.4f}\")\n",
        "print(f\"Recall:    {eval_results.get('eval_recall', 'N/A'):.4f}\")\n",
        "print(f\"F1-Score:  {eval_results.get('eval_f1', 'N/A'):.4f}\")\n",
        "print(f\"Accuracy:  {eval_results.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "print(\"----------------------------------------------------\")\n",
        "\n",
        "trainer.log_metrics(\"eval_test\", eval_results)\n",
        "trainer.save_metrics(\"eval_test\", eval_results)\n",
        "\n",
        "print(\"Baseline model training and evaluation complete!\")"
      ],
      "metadata": {
        "id": "76cfN_wlIKth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}